{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"529554fc1039eef87120a6eb78cb8d0f289f7339"},"source":["This notebook covers:\n","* Regex basics\n","* Web scraping with BeautifulSoup"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8587b847b721482289161fc591509eca68cf3315","collapsed":true,"trusted":false},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"10cd97bfa26f84442f0e32a26b01cf8b6c9169b4"},"source":["## Regular expressions"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"cf339878b60841ce5bc5491767b8753c04c377ab","trusted":true},"outputs":[],"source":["from IPython.display import Image"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"47bb40cbd660f2425e61e232bb1f7143707f9839"},"source":["I have talked about some basic regex functionality which is taken from this excellent post"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"537f565d77cec29e570c510b9d7ffbed2f0f9d01"},"source":["https://www.machinelearningplus.com/python/python-regex-tutorial-examples/"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c026c0a1fb31784c477a81ab9d0555b522634e75","collapsed":true,"trusted":false},"outputs":[],"source":["import re"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"570239c59df6d2c245d5fbb5a26011e3ddb2c18e","collapsed":true,"trusted":true},"outputs":[],"source":["Image(\"../imgs/regex.png\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"b8dc504876b8d816d4fa7bbbd39443920a704f14"},"source":["A regex pattern is a special language used to represent generic text, numbers or symbols so it can be used to extract texts that conform to that pattern."]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"ad0db29e249dd9ffb03fa85dabe1e7a021c3b647"},"source":["Here the '\\s' matches any whitespace character. By adding a '+' notation at the end will make the pattern match at least 1 or more spaces. So, this pattern will match even tab '\\t' characters as well."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7432d095972ee0c4d5c0823799459cfd1abf0a14","collapsed":true,"trusted":false},"outputs":[],"source":["regex = re.compile('\\s+')"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"bd9fd8c41a4125219b20a8b48176b61b1973de20"},"source":["**Splitting a string using regex**"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"da5783b46154ba18400b8452da89bbab1de7682c","collapsed":true,"trusted":false},"outputs":[],"source":["text = \"Hello World.   Regex is awesome\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\" \".join(text.split())"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"fa93b6c42156a7550438d8fe6e395f50436a61d8","trusted":false},"outputs":[],"source":["regex.split(text)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"f44fdf9f491f748456037c3fcc75bba51a32cac6"},"source":["Another way but regex is generally the better one"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"184e8dadf759664c110333ce90a1c1ec2f4508e8","trusted":false},"outputs":[],"source":["re.split('\\s', text)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"a01a271f7f843c4176bc5f9ef55dcf259e429d2a"},"source":["**re.findall**"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"bee96336be13efe4676c8ed9f5bd4056333a0e38"},"source":["the findall method extracts all occurrences of the pattern"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"c10a03e40f667741f023082276539aba57ce87bd"},"source":[" `'\\d'` is a regular expression which matches any digit"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e545e0deec686e3867028f0b4ffe9bc0516e8b48","collapsed":true,"trusted":false},"outputs":[],"source":["text = \"101 howard street, 246 mcallister street\""]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1a3fc378d262e747def16fc65c163687d58e5830","collapsed":true,"trusted":false},"outputs":[],"source":["regex_num = re.compile('\\d+')  #one or more digits"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d922d09aa08dab31a0b5fe345b8c7f38e61c871e","trusted":false},"outputs":[],"source":["regex_num.findall(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ae59445e48ed013be32b7c75e4ce0bfab6d0880c","trusted":false},"outputs":[],"source":["regex_num.split(text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["re.findall('\\d+', text)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"37a63ea8747b63f9d4dcb7dee6b8bc28b2be989d"},"source":["**re.search() vs re.match()**"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"07666b59e0f2486ad70a279f641c93f1268d9972"},"source":["`regex.search()` returns a particular match object that contains the starting and ending positions of the **first occurrence of the pattern**.\n","\n","Likewise, `regex.match()` also returns a match object. But the difference is, it requires the pattern to be present at the **beginning of the text itself**."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"000af117eb4cc58657df8dbd738ad8a12a4fd39d","collapsed":true,"trusted":false},"outputs":[],"source":["text2 = \"MAT 20567567576  Mathematics 189\""]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c1315a35cb1a9fce4a2fca8822f302196dfc0f1e","collapsed":true,"trusted":false},"outputs":[],"source":["m = regex_num.match(text2)\n","m"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1c7435e9191c13d3ea4faf8bf1280c7c3d2101ce","trusted":false},"outputs":[],"source":["m.group()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"936d2fc8accc08632e11be2b3b7898fbcac60b1f","trusted":false},"outputs":[],"source":["m.start()  #returns the index of the starting"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1b7a53f156886dc875b1f1ba4f308f2f6347425b","collapsed":true,"trusted":false},"outputs":[],"source":["s = regex_num.search(text2)\n","s"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2f0d615a4ce19f36abb842dcd21663e37a3d9fd6","trusted":false},"outputs":[],"source":["s.group()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"c7ccd41cc77c525e8ae5eaf63e8673e0af1a75b9"},"source":["**Substituting one text by another using `regex.sub()`**"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"bd54698c0eac18f68da6bbdcf3156318acc57689","collapsed":true,"trusted":false},"outputs":[],"source":["text = \"\"\"101   COM \\t  Computers\n","205   MAT \\t  Mathematics\n","189   ENG  \\t  English\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"bf8e1047edc05e645a2c1d1b25f087403a76295c","collapsed":true,"trusted":false},"outputs":[],"source":["regex = re.compile('\\s+')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9fef5a9568aed119070b27d7adceefe7491f761d","trusted":false},"outputs":[],"source":["regex.sub(' ', text)  #it replaces the regular expression by ' '"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ce5af2862542536ecfd77ff4bd2946256b3616cb","trusted":false},"outputs":[],"source":["# get rid of all extra spaces except newline\n","regex = re.compile('((?!\\n)\\s+)')\n","print(regex.sub(' ', text))"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"87043e4df8463e672501cc0f58aa7e2fe9ec529d"},"source":["**combining regex pattern**"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8fe182fbc250bdc8863817ca62a626a82d13aa2a","trusted":false},"outputs":[],"source":["# define the course text pattern groups and extract\n","course_pattern = '([0-9]+)\\s*([A-Z]{3})\\s*([A-Za-z]{4,})'\n","re.findall(course_pattern, text)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"4d3038458e2dce9f1fe35e188764773fe9d35baf"},"source":["**greedy regex**"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"142d7dd8e5ab1214a3d06e91c95296937af308f8"},"source":["The default behavior of regular expressions is to be greedy. That means it tries to extract as much as possible until it conforms to a pattern even when a smaller part would have been syntactically sufficient."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e4870f6a9c7d774811790af2309fac2d8aadba54","trusted":false},"outputs":[],"source":["text = \"< body>Regex Greedy Matching Example < /body>\"\n","re.findall('<.*>', text)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"e2738db5df307cde0f061c5f35b03e7683a1b82f"},"source":["it should have stopped at first > but it didn't. For extracting only the smaller portions:"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"537b2d90e9ede94123561b3b73435d91b2cae282"},"source":["Lazy matching, on the other hand, ‘takes as little as possible’. This can be effected by adding a `?` at the end of the pattern."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f6b670ef79d7901dfde0fd98c64cf4d6a290ea48","trusted":false},"outputs":[],"source":["re.findall('<.*?>', text)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"00f8d96d129458da810b886e275e8b372a64bcbb","collapsed":true,"trusted":false},"outputs":[],"source":["s = re.search('<.*?>', text)  #getting only the first one"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3524f83ab553275242b47ff7c80d9efcf102c4db","trusted":false},"outputs":[],"source":["s.group()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"675278d128fbf7dec7b272316d6dd308608fbcae","collapsed":true,"trusted":false},"outputs":[],"source":["text = '01, Jan 2015'"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e13e268bc8c72299fdc78d3a4ffe2afcd4bc8e34","trusted":false},"outputs":[],"source":["print(re.findall('\\d{3}', text))"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"99308052f50445899ecfef3d86737dc459dd7644"},"source":["**matching word boundaries**"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"1e96e43cfeaec6475e5163306f589ddfee470a24"},"source":["Word boundaries `\\b` are commonly used to detect and match the beginning or end of a word. That is, one side is a word character and the other side is whitespace and vice versa.\n","\n","For example, the regex \\btoy will match the ‘toy’ in ‘toy cat’ and not in ‘tolstoy’. In order to match the ‘toy’ in ‘tolstoy’, you should use toy\\b\n","\n","Can you come up with a regex that will match only the first ‘toy’ in ‘play toy broke toys’? (hint: \\b on both sides)\n","\n","Likewise, `\\B` will match any non-boundary.\n","\n","For example, \\Btoy\\B will match ‘toy’ surrounded by words on both sides, as in, ‘antoynet’."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f72bed4462e57db41e860fb74b9c034ef0885522","trusted":false},"outputs":[],"source":["re.findall(r'\\btoy\\b', 'play toy broke toys')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8ded6ca9cccbb390724ed2e515a93a09be700d05","trusted":false},"outputs":[],"source":["re.findall(r'\\btoy', 'play toy broke toys')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2e464fb5be59c819a92cec2525055c2981a8d2b6","trusted":false},"outputs":[],"source":["re.findall(r'toy\\b', 'play toy broke toys')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"65157a29d9124ea4118af3f24313744928c4aaa3","trusted":false},"outputs":[],"source":["re.findall(r'\\Btoy\\b', 'playtoy broke toys')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4e33eb2c05cce0d7c5a869acd9c80c3d2b1a4349","trusted":false},"outputs":[],"source":["re.findall(r'\\Btoy\\B', 'playtoybroke toys')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"cf4f08f4741a20e6b7f43ae2b0f93440e1b9e842","trusted":false},"outputs":[],"source":["re.findall(r'\\btoy', 'playtoybroke toys')"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"7d0b8c845660977871b8f2b7e0abfe1023dbe773"},"source":["**Practice regex examples**"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"199f74520e95b660a6d3323bf2b45146013cb1ac","collapsed":true,"trusted":false},"outputs":[],"source":["emails = \"\"\"zuck26@facebook.com\n","page33@google.com\n","jeff42@amazon.com\"\"\"\n","\n","desired_output = [('zuck26', 'facebook', 'com'), ('page33', 'google', 'com'),\n","                  ('jeff42', 'amazon', 'com')]"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6518108745df7697ac907fe7f218cb44cf1e7b78","collapsed":true,"trusted":false},"outputs":[],"source":["regex = re.compile('([\\w]+)@([\\w]+).([\\w]+)')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7abc539508806923bbfc22b953099e849c92c55e","trusted":false},"outputs":[],"source":["regex.findall(emails)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"17cd6968a3384e66c6c318f978f3fef8bd79fa6e"},"source":["2. Retrieve all the words starting with ‘b’ or ‘B’ from the following text."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"452f6dfed9a2194ad5b0680363cc587b35e86c19","collapsed":true,"trusted":false},"outputs":[],"source":["text = \"\"\"Betty bought a bit of butter, \n","But the butter was so bitter, So she bought\n","some better butter, To make the bitter butter better.\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"92850ccf1845a30e958bdd6929d59569d8af2738","collapsed":true,"trusted":false},"outputs":[],"source":["regex = re.compile('([$bB]\\w+)')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5c3da9b9214e8a2b607155936089fdb1080c81d0","trusted":false},"outputs":[],"source":["regex.findall(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9cd8b2cd6df3300ff47653822e4ae34805cf7bac","collapsed":true,"trusted":false},"outputs":[],"source":["sentence = \"\"\"A, very   very; irregular_sentence\"\"\"\n","desired_output = \"A very very irregular sentence\""]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8cb82c7c0f30abcc40f04767390f170abb3f8f50","collapsed":true,"trusted":false},"outputs":[],"source":["regex = re.compile('[,\\s;_]+')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b08fa1c94763b29c9d5c2462989f3928739dc4f8","trusted":false},"outputs":[],"source":["' '.join(regex.split(sentence))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f9d06204f9fbfa793bdf341760bb80e14e47ba1c","collapsed":true,"trusted":false},"outputs":[],"source":["tweet = '''Good advice! RT @TheNextWeb: What I would do differently if I was learning to code today http://t.co/lbwej0pxOd cc: @garybernhardt #rstats'''"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"43e5e0cd9cb9bb9eb0c5cdb6959904e0c3c1259a","collapsed":true,"trusted":false},"outputs":[],"source":["desired_output = 'Good advice What I would do differently if I was learning to code today'"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"86a67db9d560a83385d09a288f8b7c588097ae13","trusted":false},"outputs":[],"source":["def clean_tweet(tweet):\n","    tweet = re.sub('http\\S+\\s*', '', tweet)  # remove URLs\n","    tweet = re.sub('RT|cc', '', tweet)  # remove RT and cc\n","    tweet = re.sub('#\\S+', '', tweet)  # remove hashtags\n","    tweet = re.sub('@\\S+', '', tweet)  # remove mentions\n","    tweet = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"),\n","                   '', tweet)  # remove punctuations\n","    tweet = re.sub('\\s+', ' ', tweet)  # remove extra whitespace\n","    return tweet\n","\n","\n","print(clean_tweet(tweet))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Web Scraping with BeautifulSoup"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"706166c8c2d8ac42e917e833d01108b73b9c198f","collapsed":true,"trusted":false},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import urllib.request\n","import requests"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def download_html(url):\n","    with urllib.request.urlopen(url) as response:\n","        html = response.read()\n","        html = html.decode('utf-8')\n","    response.close()\n","    return html"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["url = 'https://www.imdb.com/search/title?release_date=2018&sort=boxoffice_gross_us,desc&start=1'\n","html = download_html(url)\n","\n","soup = BeautifulSoup(html, 'lxml') #html.parser"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Since above code extracts all data on the first page, below code is run only to extract movie information on it."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["movie_blocks = soup.findAll('div',  {'class':'lister-item-content'})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["movie_blocks[0]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["BeautifulSoup.find_all(arguments) returns a list of BeautifulSoup objects. These are all occurrences matching the arguments. If there are no matches, method returns empty list. This is obviously used, when you cannot identify it right away and have to do some more digging before you get to the data you want.\n","\n","> Let's examine one of the extracted block to identify the elements that we need to scrape."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["movie_blocks[0].find('span', {'class': 'lister-item-year'}).contents"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","year = re.compile(\"\\d+\")\n","year.search(movie_blocks[0].find('span',{'class': 'lister-item-year'}).contents[0]).group()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mname = movie_blocks[0].find('a').get_text() # Name of the movie\n","\n","m_reyear = int(movie_blocks[0].find('span',{'class': 'lister-item-year'}).contents[0][1:-1]) # Release year\n","\n","m_rating = float(movie_blocks[0].find('div',{'class':'inline-block ratings-imdb-rating'}).get('data-value')) #rating\n","\n","m_mscore = float(movie_blocks[0].find('span',{'class':'metascore favorable'}).contents[0].strip()) #meta score\n","\n","m_votes = int(movie_blocks[0].find('span',{'name':'nv'}).get('data-value')) # votes\n","\n","print(\"Movie Name: \" + mname,\n","      \"\\nRelease Year: \" + str(m_reyear),\n","      \"\\nIMDb Rating: \" + str(m_rating),\n","      \"\\nMeta score: \" + str(m_mscore),\n","      \"\\nVotes: \" + '{:,}'.format(m_votes)\n","\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def scrape_mblock(movie_block):\n","    \n","    movieb_data ={}\n","  \n","    try:\n","        movieb_data['name'] = movie_block.find('a').get_text() # Name of the movie\n","    except:\n","        movieb_data['name'] = None\n","\n","    try:    \n","        movieb_data['year'] = str(movie_block.find('span',{'class': 'lister-item-year'}).contents[0][1:-1]) # Release year\n","    except:\n","        movieb_data['year'] = None\n","\n","    try:\n","        movieb_data['rating'] = float(movie_block.find('div',{'class':'inline-block ratings-imdb-rating'}).get('data-value')) #rating\n","    except:\n","        movieb_data['rating'] = None\n","    \n","    try:\n","        movieb_data['m_score'] = float(movie_block.find('span',{'class':'metascore favorable'}).contents[0].strip()) #meta score\n","    except:\n","        movieb_data['m_score'] = None\n","\n","    try:\n","        movieb_data['votes'] = int(movie_block.find('span',{'name':'nv'}).get('data-value')) # votes\n","    except:\n","        movieb_data['votes'] = None\n","\n","    return movieb_data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["> Then I create the below function to scrape all movie blocks within a single search result page"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def scrape_m_page(movie_blocks):\n","    \n","    page_movie_data = []\n","    num_blocks = len(movie_blocks)\n","    \n","    for block in range(num_blocks):\n","        page_movie_data.append(scrape_mblock(movie_blocks[block]))\n","    \n","    return page_movie_data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["> Now we built functions to extract all movie data from a single page.\n","\n","Next function will be created to iterate the above made function through all pages of the search result untill we scrape data for the targeted number of movies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import time     \n","import random as ran\n","def scrape_this(link,t_count):\n","    \n","    #from IPython.core.debugger import set_trace\n","\n","    base_url = link\n","    target = t_count\n","    \n","    current_mcount_start = 0\n","    current_mcount_end = 0\n","    remaining_mcount = target - current_mcount_end \n","    \n","    new_page_number = 1\n","    \n","    movie_data = []\n","    \n","    \n","    while remaining_mcount > 0:\n","\n","        url = base_url + str(new_page_number)\n","        \n","        #set_trace()\n","        \n","        source = download_html(url)\n","        soup = BeautifulSoup(source, 'html.parser') # lxml\n","        \n","        movie_blocks = soup.findAll('div',{'class':'lister-item-content'})\n","        \n","        movie_data.extend(scrape_m_page(movie_blocks))   \n","        \n","        current_mcount_start = int(soup.find(\"div\", {\"class\":\"nav\"}).find(\"div\", {\"class\": \"desc\"}).contents[1].get_text().split(\"-\")[0])\n","\n","        current_mcount_end = int(soup.find(\"div\", {\"class\":\"nav\"}).find(\"div\", {\"class\": \"desc\"}).contents[1].get_text().split(\"-\")[1].split(\" \")[0])\n","\n","        remaining_mcount = target - current_mcount_end\n","        \n","        print('\\r' + \"currently scraping movies from: \" + str(current_mcount_start) + \" - \"+str(current_mcount_end), \"| remaining count: \" + str(remaining_mcount), flush=True, end =\"\")\n","        \n","        new_page_number = current_mcount_end + 1\n","        \n","        time.sleep(ran.randint(0, 10))\n","    \n","    return movie_data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["> Finally, we have put together all functions created above to scrape the top 150 movies on the list"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd \n","base_scraping_link = \"https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=boxoffice_gross_us,desc&start=\"\n","\n","top_movies = 150 #input(\"How many movies do you want to scrape?\")\n","films = []\n","\n","movies = scrape_this(base_scraping_link,int(top_movies))\n","\n","print('\\r'+\"List of top \" + str(top_movies) +\" movies:\" + \"\\n\", end=\"\\n\")\n","movies=pd.DataFrame(movies)\n","movies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["movies.to_csv('../datasets/movies.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"toc_cell":true,"toc_position":{},"toc_section_display":"none","toc_window_display":false}},"nbformat":4,"nbformat_minor":1}
